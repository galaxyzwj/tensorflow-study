{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl #画图用的库\n",
    "import matplotlib.pyplot as plt\n",
    "#下面这一句是为了可以在notebook中画图\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn   #机器学习算法库\n",
    "import pandas as pd #处理数据的库   \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras   #使用tensorflow中的keras\n",
    "#import keras #单纯的使用keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, sklearn, pd, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist # 该数据集是黑白服装数据集\n",
    "#拆分训练集和测试集\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "#将训练集拆分为训练集和验证集\n",
    "#训练集共6万张图片，我们将前5000张作为验证集，后面所有的做训练集\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "print(x_train[0].dtype)\n",
    "print(x_train[0]) # 是一个数据矩阵 28*28, 矩阵中的每一个数值都是uint8类型\n",
    "print(y_train[0]) #这里的y值均为数字编码，非向量，所以后面定义模型损失函数为 sparse_categorical_crossentropy\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在图像分类领域我们提升准确率的手段 归一化：\n",
    "# 1.对训练数据进行归一化 2. 批归一化\n",
    "\n",
    "# x = (x - u)/std  u为均值，std为方差\n",
    "from sklearn.preprocessing import StandardScaler #使用sklearn中的StandardScaler实现训练数据归一化\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#fit_transform:得到方差、均值、最大最小值然后数据进行归一化操作\n",
    "#https://blog.csdn.net/youhuakongzhi/article/details/90519801\n",
    "#x_train：先转为float32用于做除法，x_train本身为三维矩阵[None,28,28]，因为fit_transform要求二维数据所以需要转换为[None, 784]，再转回四维矩阵\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "#是因为在trainData的时候，已经使用fit()得到了整体的指标（均值，方差等）并被保存起来了后面验证集测试集可以使用，所以在测试集上直接transform()，使用之前的指标，\n",
    "#如果在测试集上再进行fit()，由于两次的数据不一样，导致得到不同的指标，会使预测发生偏差，因为模型是针对之前的数据fit()出来\n",
    "#的标准来训练的，而现在的数据是新的标准，会导致预测的不准确\n",
    "x_valid_scaled = scaler.transform(x_valid.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "x_test_scaled = scaler.transform(x_test.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "#reshape(-1,1)表示（任意行，1列），这里个人认为设置里面什么参数影响不大，只要是转换为二维即可，反正最终要转换为三/四 维使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tf.keras.models.Sequential()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "\n",
    "#使用深度卷积网络实现\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(10):\n",
    "    model.add(keras.layers.Dense(100,activation=\"selu\"))# 激活函数selu自带数据归一化功能，在一定程度上也能缓解梯度消失问题\n",
    "\n",
    "'''\n",
    "#使用卷积神经网络实现\n",
    "#激活函数这里使用了自带批归一化的selu函数来代替使用relu激活函数\n",
    "model.add(keras.layers.Conv2D(filters=32,kernel_size=3,padding='same',activation=\"selu\",input_shape=(28, 28, 1)))\n",
    "model.add(keras.layers.Conv2D(filters=32,kernel_size=3,padding='same',activation=\"selu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=3,padding='same',activation=\"selu\"))\n",
    "model.add(keras.layers.Conv2D(filters=64,kernel_size=3,padding='same',activation=\"selu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=\"selu\"))\n",
    "model.add(keras.layers.Conv2D(filters=128,kernel_size=3,padding='same',activation=\"selu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation=\"selu\"))\n",
    "'''\n",
    "\n",
    "#softmax层输出\n",
    "model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"adam\", #optimizer=\"sgd\", 优化算法一般来说我们无脑用adam即可\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "#查看上面建立的模型架构信息\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit用于训练\n",
    "history=model.fit(x_train_scaled, y_train, epochs=10, #epochs用于遍历训练集次数\n",
    "                  validation_data=(x_valid_scaled,y_valid),#加入验证集，每隔一段时间就对验证集进行验证\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将上面history中的数据指标用一张图来表示\n",
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5)) #设置图的大小\n",
    "    plt.grid(True) #显示网格\n",
    "    plt.gca().set_ylim(0,1) #设置y轴范围\n",
    "    plt.show()\n",
    "plot_learning_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#测试集上进行测试评估一下\n",
    "model.evaluate(x_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model,'./keras_saved_graph')#保存 savedModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir ./keras_saved_graph --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir ./keras_saved_graph --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli run --dir ./keras_saved_graph --tag_set serve --signature_def serving_default --input_exprs 'flatten_1_input=np.ones((2,28,28))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_saved_model = tf.saved_model.load('./keras_saved_graph/')#加载本地的模型\n",
    "print(list(loaded_saved_model.signatures.keys()))#打印所有的模型签名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过上面的签名获取一个函数句柄inference\n",
    "inference = loaded_saved_model.signatures['serving_default']\n",
    "print(inference)\n",
    "print(inference.structured_outputs)#输出dense_21,与前面的saved_model_cli命令输出一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用inference对象来做一个推理\n",
    "resultes = inference(tf.constant(x_test_scaled[0:1]))#向这个句柄中传入test集合中的第一个图像输入\n",
    "print(resultes)#输出为一个字典\n",
    "print(resultes['dense_21'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
