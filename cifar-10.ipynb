{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.18.0\n",
      "sklearn 0.21.3\n",
      "pandas 0.25.3\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl #画图用的库\n",
    "import matplotlib.pyplot as plt\n",
    "#下面这一句是为了可以在notebook中画图\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn   #机器学习算法库\n",
    "import pandas as pd #处理数据的库   \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow import keras   #使用tensorflow中的keras\n",
    "#import keras #单纯的使用keras\n",
    " \n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, sklearn, pd, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[('/home/galaxy/DeepLearning/DATASETS/cifar-10/train/1.png', 'frog'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/train/2.png', 'truck'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/train/3.png', 'truck'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/train/4.png', 'deer'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/train/5.png', 'automobile')]\n",
      "[('/home/galaxy/DeepLearning/DATASETS/cifar-10/test/1.png', 'cat'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/test/2.png', 'cat'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/test/3.png', 'cat'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/test/4.png', 'cat'),\n",
      " ('/home/galaxy/DeepLearning/DATASETS/cifar-10/test/5.png', 'cat')]\n",
      "50000 300000\n"
     ]
    }
   ],
   "source": [
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',    \n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',   \n",
    "]\n",
    "train_dir         = \"/home/galaxy/DeepLearning/DATASETS/cifar-10/train\"\n",
    "train_labels_file = \"/home/galaxy/DeepLearning/DATASETS/cifar-10/trainLabels.csv\"\n",
    "test_dir          = \"/home/galaxy/DeepLearning/DATASETS/cifar-10/test\"\n",
    "test_csv_file     = '/home/galaxy/DeepLearning/DATASETS/cifar-10/sampleSubmission.csv'\n",
    "\n",
    "print(os.path.exists(train_dir))\n",
    "print(os.path.exists(test_dir))\n",
    "print(os.path.exists(train_labels_file))\n",
    "\n",
    "#读取csv文件查看其内容\n",
    "#labels = pd.read_csv(train_labels_file, header=0)\n",
    "#print(labels)\n",
    "\n",
    "#因为train文件夹下直接存放的是所有的文件，其每个图片的编号对应的label都在csv文件里面一一对应，所以我们需要将图片文件与label一一对应\n",
    "def parse_csv_file(filepath, folder):\n",
    "    results = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()[1:]#跳过 第一行的 id,label\n",
    "    for line in lines:\n",
    "        image_id, label_str = line.strip('\\n').split(',')#strip表示去掉换行符，以 ','作为分隔符\n",
    "        image_full_path = os.path.join(folder, image_id + '.png')\n",
    "        results.append((image_full_path, label_str))\n",
    "    return results\n",
    "\n",
    "import pprint\n",
    "train_labels_info = parse_csv_file(train_labels_file, train_dir)\n",
    "test_labels_info  = parse_csv_file(test_csv_file, test_dir)\n",
    "\n",
    "pprint.pprint(train_labels_info[0:5])\n",
    "pprint.pprint(test_labels_info[0:5])\n",
    "\n",
    "print(len(train_labels_info), len(test_labels_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filepath       class\n",
      "0  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...        frog\n",
      "1  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...       truck\n",
      "2  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...       truck\n",
      "3  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...        deer\n",
      "4  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...  automobile\n",
      "                                            filepath       class\n",
      "0  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...       horse\n",
      "1  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...  automobile\n",
      "2  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...        deer\n",
      "3  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...  automobile\n",
      "4  /home/galaxy/DeepLearning/DATASETS/cifar-10/tr...    airplane\n",
      "                                            filepath class\n",
      "0  /home/galaxy/DeepLearning/DATASETS/cifar-10/te...   cat\n",
      "1  /home/galaxy/DeepLearning/DATASETS/cifar-10/te...   cat\n",
      "2  /home/galaxy/DeepLearning/DATASETS/cifar-10/te...   cat\n",
      "3  /home/galaxy/DeepLearning/DATASETS/cifar-10/te...   cat\n",
      "4  /home/galaxy/DeepLearning/DATASETS/cifar-10/te...   cat\n"
     ]
    }
   ],
   "source": [
    "#train_df = pd.DataFrame(train_labels_info)#DataFrame 为 表格型数据结构\n",
    "#这里将train_df切分为 训练集和验证集 两部分\n",
    "train_df = pd.DataFrame(train_labels_info[0:45000])\n",
    "valid_df = pd.DataFrame(train_labels_info[45000:])\n",
    "test_df  = pd.DataFrame(test_labels_info)\n",
    "\n",
    "#设置 DataFrame的列名\n",
    "train_df.columns = ['filepath', 'class']\n",
    "valid_df.columns = ['filepath', 'class']\n",
    "test_df.columns  = ['filepath', 'class']\n",
    "\n",
    "print(train_df.head())\n",
    "print(valid_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45000 validated image filenames belonging to 10 classes.\n",
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Found 5000 validated image filenames belonging to 10 classes.\n",
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "45000 5000\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "#因为这里的数据集不是按照文件夹来分类的，而是所有的图片数据都是在一个文件夹中，我们使用DataFrame来表示每一个数据文件对应的label，\n",
    "#所以这里使用的是 flow_from_dataframe\n",
    "##################################\n",
    "\n",
    "#resnet50使用的图像宽高均为224\n",
    "#height      = 224\n",
    "#width       = 224\n",
    "height      = 32 #设置图像被缩放的宽高\n",
    "width       = 32\n",
    "channels    = 3   #图像通道数\n",
    "batch_size  = 32\n",
    "num_classes = 10\n",
    "\n",
    "##########------------训练集数据------------##########\n",
    "#初始化一个训练数据相关的generator\n",
    "#具体用于 数据集中的图片数据进行处理，可以对图片数据进行归一化、旋转、翻转等数据增强类操作\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #preprocessing_function = keras.applications.resnet50.preprocess_input,#resnet50专门用来预处理图像的函数，把图像做归一化到-1~1之间\n",
    "    # 使用第一行preprocessing_function 就不需要 rescale\n",
    "    rescale           = 1./255, #放缩因子, 除以255是因为图片中每个像素点值范围都在0~255之间\n",
    "    rotation_range    = 40,  #图片随机转动的角度范围(-40 ~ 40)\n",
    "    width_shift_range = 0.2, #值 < 1时，表示偏移的比例，即在 0～值 这个比例幅度之间进行偏移\n",
    "    height_shift_range= 0.2, #值 > 1时，表示像素宽度，即该图片的偏移幅度大小\n",
    "    shear_range       = 0.2, #剪切强度\n",
    "    zoom_range        = 0.2, #缩放强度\n",
    "    horizontal_flip   = True,#水平随机翻转\n",
    "    fill_mode         = 'nearest',#像素填充模式\n",
    ")\n",
    "#接下来读取目录下的图片然后按照上面的数据增强相关操作对图片进行处理\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df,\n",
    "                                                    directory=train_dir,\n",
    "                                                    x_col='filepath',\n",
    "                                                    y_col='class',\n",
    "                                                    classes=class_names,\n",
    "                                                    target_size=(height,width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    seed=7,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='sparse')\n",
    "'''\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                    target_size = (height,width), #目录下的图片会被resize的大小\n",
    "                                                    batch_size  = batch_size,\n",
    "                                                    seed        = 7,#随机种子，用于洗牌和转换，随便给个数即可\n",
    "                                                    shuffle     = True,#False->则按字母数字顺序对数据进行排序 True->打乱数据\n",
    "                                                    class_mode  = \"categorical\", # 该参数决定了返回的标签数组的形式\n",
    "                                                    #classes     = 这个参数就是描述的 文件夹名与输出标签的对应关系\n",
    "                                                   )\n",
    "'''\n",
    "#classes：可选参数,为子文件夹的列表,如['dogs','cats']默认为None. 若未提供,则该类别列表将从directory下的子文件夹名称/结构自动推断。\n",
    "#每一个子文件夹都会被认为是一个新的类。(类别的顺序将按照字母表顺序映射到标签值)。通过属性class_indices可获得文件夹名与类的序号的对应字典。\n",
    "\n",
    "#使用生成器的.class_indices方法即可获取模型默认的Labels序列，文件夹名与类的序号的对应字典\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "\n",
    "##########------------验证集数据------------##########\n",
    "#初始化一个验证数据相关的generator\n",
    "#验证数据集上不需要进行数据增强的相关操作，仅保留缩放即可，不然的话训练集与验证集的值的分布会不同\n",
    "valid_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #preprocessing_function = keras.applications.resnet50.preprocess_input,#resnet50专门用来预处理图像的函数，相当于归一化，所以无需rescale\n",
    "    rescale           = 1./255, #放缩因子, 除以255是因为图片中每个像素点值范围都在0~255之间\n",
    ")\n",
    "#接下来读取目录下的图片然后按照上面的数据增强相关操作对图片进行处理\n",
    "valid_generator = valid_datagen.flow_from_dataframe(valid_df,\n",
    "                                                    directory=train_dir,\n",
    "                                                    x_col='filepath',\n",
    "                                                    y_col='class',\n",
    "                                                    classes=class_names,\n",
    "                                                    target_size=(height,width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    seed=7,\n",
    "                                                    shuffle=True,\n",
    "                                                    class_mode='sparse')\n",
    "'''\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, \n",
    "                                                    target_size = (height,width), #目录下的图片会被resize的大小\n",
    "                                                    batch_size  = batch_size,\n",
    "                                                    seed        = 7,#随机种子，用于洗牌和转换，随便给个数即可\n",
    "                                                    shuffle     = False,#不需要训练所以不需要打乱数据\n",
    "                                                    class_mode  = \"categorical\", # 该参数决定了返回的标签数组的形式\n",
    "                                                    )\n",
    "'''\n",
    "\n",
    "#使用生成器的.class_indices方法即可获取模型默认的Labels序列，文件夹名与类的序号的对应字典\n",
    "print(valid_generator.class_indices)\n",
    "\n",
    "train_num = train_generator.samples\n",
    "valid_num = valid_generator.samples\n",
    "print(train_num, valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 32, 3) (32,)\n",
      "[2. 1. 4. 4. 4. 4. 6. 5. 2. 8. 4. 6. 6. 3. 7. 1. 7. 2. 8. 8. 3. 0. 5. 3.\n",
      " 9. 1. 4. 5. 6. 7. 9. 2.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    x,y = train_generator.next()\n",
    "    print(x.shape, y.shape)\n",
    "    print(y)\n",
    "#因为 class_mode 设置为sparse，所以label标签返回的是2D的one-hot编码标签(2-> one_hot -> [0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 128)       3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 8,783,498\n",
      "Trainable params: 8,779,914\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#使用resnet50做迁移学习\n",
    "\n",
    "'''\n",
    "#1.这里ResNet50层当做一层，只有最后一层是可以被训练的\n",
    "resnet50_fine_tune = keras.models.Sequential([\n",
    "    keras.applications.ResNet50(include_top=False,#include_top：是否保留顶层的全连接网络,这里最后要定义自己的softmax选False\n",
    "                                pooling='avg',#‘avg’代表全局平均池化，‘max’代表全局最大值池化\n",
    "                                weights='imagenet',#None代表随机初始化，即不加载预训练权重；'imagenet’代表加载预训练权重\n",
    "                               ),\n",
    "    keras.layers.Dense(num_classes, activation='softmax'),    \n",
    "])\n",
    "resnet50_fine_tune.layers[0].trainable=False #设置ResNet50这一层的参数不可训练，因为 weights='imagenet'\n",
    "\n",
    "\n",
    "#2.这里ResNet50中最后几层都是可以训练,我们可以在模型架构里面看到 Trainable params可训练参数会大大增加\n",
    "resnet50 = keras.applications.ResNet50(include_top=False, pooling='avg', weights='imagenet')\n",
    "for layers in resnet50.layers[0:-5]: #这里遍历最后五层之前的layers并设置其权重相关参数不可遍历\n",
    "    layers.trainable = False\n",
    "\n",
    "resnet50_fine_tune = keras.models.Sequential([\n",
    "    resnet50,\n",
    "    keras.layers.Dense(num_classes, activation='softmax'),    \n",
    "])\n",
    "'''\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\",input_shape=(width, height, channels)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),    \n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),    \n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512,activation=\"relu\"),\n",
    "    keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "#损失函数 sparse_categorical_crossentropy 和 categorical_crossentropy 的选择取决于前面设定的y值的取值类型\n",
    "#如果y取值为 2D的 one-hot编码，则选择 categorical_crossentropy\n",
    "#如果y取值为 1D的 整数标签，则选择 sparse_categorical_crossentropy\n",
    "#前面的 tensorflow2------分类问题fashion_mnist 文章中有过相关描述\n",
    "# metrics 表示选择 accuracy作为评价参数\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1406/1406 [==============================] - 113s 81ms/step - loss: 2.6016 - accuracy: 0.2684 - val_loss: 1.6631 - val_accuracy: 0.3770\n",
      "Epoch 2/20\n",
      "1406/1406 [==============================] - 106s 75ms/step - loss: 1.6822 - accuracy: 0.3834 - val_loss: 2.3701 - val_accuracy: 0.3215\n",
      "Epoch 3/20\n",
      "1406/1406 [==============================] - 107s 76ms/step - loss: 1.4986 - accuracy: 0.4571 - val_loss: 1.4432 - val_accuracy: 0.5016\n",
      "Epoch 4/20\n",
      "1406/1406 [==============================] - 117s 83ms/step - loss: 1.3415 - accuracy: 0.5170 - val_loss: 1.3990 - val_accuracy: 0.5363\n",
      "Epoch 5/20\n",
      "1406/1406 [==============================] - 110s 78ms/step - loss: 1.1871 - accuracy: 0.5793 - val_loss: 1.2035 - val_accuracy: 0.5911\n",
      "Epoch 6/20\n",
      "1406/1406 [==============================] - 117s 83ms/step - loss: 1.0670 - accuracy: 0.6258 - val_loss: 0.9764 - val_accuracy: 0.6809\n",
      "Epoch 7/20\n",
      "1406/1406 [==============================] - 114s 81ms/step - loss: 0.9616 - accuracy: 0.6647 - val_loss: 1.0508 - val_accuracy: 0.6653\n",
      "Epoch 8/20\n",
      "1406/1406 [==============================] - 113s 80ms/step - loss: 0.8836 - accuracy: 0.6950 - val_loss: 0.7605 - val_accuracy: 0.7526\n",
      "Epoch 9/20\n",
      "1406/1406 [==============================] - 113s 81ms/step - loss: 0.8317 - accuracy: 0.7132 - val_loss: 0.9704 - val_accuracy: 0.6939\n",
      "Epoch 10/20\n",
      "1406/1406 [==============================] - 113s 81ms/step - loss: 0.7763 - accuracy: 0.7325 - val_loss: 0.8582 - val_accuracy: 0.7454\n",
      "Epoch 11/20\n",
      "1406/1406 [==============================] - 110s 78ms/step - loss: 0.7274 - accuracy: 0.7518 - val_loss: 0.6818 - val_accuracy: 0.7748\n",
      "Epoch 12/20\n",
      "1406/1406 [==============================] - 110s 78ms/step - loss: 0.6933 - accuracy: 0.7635 - val_loss: 0.6644 - val_accuracy: 0.7851\n",
      "Epoch 13/20\n",
      "1406/1406 [==============================] - 108s 77ms/step - loss: 0.6585 - accuracy: 0.7753 - val_loss: 0.7298 - val_accuracy: 0.7716\n",
      "Epoch 14/20\n",
      "1406/1406 [==============================] - 111s 79ms/step - loss: 0.6355 - accuracy: 0.7832 - val_loss: 0.6333 - val_accuracy: 0.7993\n",
      "Epoch 15/20\n",
      "1406/1406 [==============================] - 110s 78ms/step - loss: 0.6077 - accuracy: 0.7918 - val_loss: 0.6512 - val_accuracy: 0.8045\n",
      "Epoch 16/20\n",
      "1406/1406 [==============================] - 111s 79ms/step - loss: 0.5785 - accuracy: 0.8014 - val_loss: 0.5360 - val_accuracy: 0.8245\n",
      "Epoch 17/20\n",
      "1406/1406 [==============================] - 114s 81ms/step - loss: 0.5589 - accuracy: 0.8105 - val_loss: 0.5277 - val_accuracy: 0.8321\n",
      "Epoch 18/20\n",
      "1406/1406 [==============================] - 112s 79ms/step - loss: 0.5376 - accuracy: 0.8154 - val_loss: 0.5789 - val_accuracy: 0.8187\n",
      "Epoch 19/20\n",
      "1406/1406 [==============================] - 116s 82ms/step - loss: 0.5219 - accuracy: 0.8227 - val_loss: 0.4822 - val_accuracy: 0.8442\n",
      "Epoch 20/20\n",
      "1406/1406 [==============================] - 112s 80ms/step - loss: 0.5136 - accuracy: 0.8259 - val_loss: 0.5538 - val_accuracy: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nhistory = resnet50_fine_tune.fit_generator(train_generator,#steps_per_epoch: 一个epoch包含的步数（每一步是一个batch的数据送入）\\n                              steps_per_epoch = train_num // batch_size,\\n                              epochs          = epochs,\\n                              validation_data = valid_generator,\\n                              validation_steps= valid_num // batch_size,\\n                              callbacks       = callbacks,\\n                             )\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "callback_dir = \"./callback_cifar-10\"\n",
    "\n",
    "if os.path.exists(callback_dir):\n",
    "    shutil.rmtree(callback_dir)\n",
    "os.mkdir(callback_dir)\n",
    "\n",
    "output_model_file=os.path.join(callback_dir,\"cifar10_model.h5\")#在logdir中创建一个模型文件.h5\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(callback_dir),\n",
    "    keras.callbacks.ModelCheckpoint(output_model_file, save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(patience=5,min_delta=1e-3),    \n",
    "]\n",
    "\n",
    "\n",
    "epochs = 20#使用fine_tune 不需要太多次迭代就能够达到一个较好的效果\n",
    "\n",
    "#使用fit_generator是因为使用的是 ImageDataGenerator 获取数据集数据的\n",
    "history = model.fit_generator(train_generator,#steps_per_epoch: 一个epoch包含的步数（每一步是一个batch的数据送入）\n",
    "                              steps_per_epoch = train_num // batch_size,\n",
    "                              epochs          = epochs,\n",
    "                              validation_data = valid_generator,\n",
    "                              validation_steps= valid_num // batch_size,\n",
    "                              callbacks       = callbacks,\n",
    "                             )\n",
    "'''\n",
    "history = resnet50_fine_tune.fit_generator(train_generator,#steps_per_epoch: 一个epoch包含的步数（每一步是一个batch的数据送入）\n",
    "                              steps_per_epoch = train_num // batch_size,\n",
    "                              epochs          = epochs,\n",
    "                              validation_data = valid_generator,\n",
    "                              validation_steps= valid_num // batch_size,\n",
    "                              callbacks       = callbacks,\n",
    "                             )\n",
    "'''\n",
    "#运行打印看到val_accuracy的值并没有逐渐变大而是一直保持不变，是因为激活函数使用的是selu导致，可尝试更换激活函数为relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300000 validated image filenames belonging to 10 classes.\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    #preprocessing_function = keras.applications.resnet50.preprocess_input,#resnet50专门用来预处理图像的函数，相当于归一化，所以无需rescale\n",
    "    rescale           = 1./255, #放缩因子, 除以255是因为图片中每个像素点值范围都在0~255之间\n",
    ")\n",
    "#接下来读取目录下的图片然后按照上面的数据增强相关操作对图片进行处理\n",
    "test_generator = test_datagen.flow_from_dataframe(test_df,\n",
    "                                                    directory=test_dir,\n",
    "                                                    x_col='filepath',\n",
    "                                                    y_col='class',\n",
    "                                                    classes=class_names,\n",
    "                                                    target_size=(height,width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    seed=7,\n",
    "                                                    shuffle=False,\n",
    "                                                    class_mode='sparse')\n",
    "test_num = test_generator.samples\n",
    "print(test_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 10)\n",
      "[[1.3304034e-01 4.6912273e-03 6.5291427e-02 2.4070667e-01 2.0920680e-01\n",
      "  2.2422984e-02 4.9592312e-02 5.5272922e-02 1.5559008e-02 2.0421635e-01]\n",
      " [3.9993563e-01 6.8166130e-03 1.7786488e-01 2.8897736e-02 2.4560432e-01\n",
      "  3.4685940e-03 6.9926865e-03 4.4956151e-03 9.0544179e-02 3.5379764e-02]\n",
      " [2.0892363e-08 9.9936479e-01 2.0792959e-10 7.7887130e-10 7.6677041e-12\n",
      "  2.9539097e-11 5.5943427e-11 1.0608656e-12 2.4068395e-08 6.3517451e-04]\n",
      " [1.7990649e-03 1.7440238e-04 1.1521533e-04 2.7537218e-04 9.4063726e-05\n",
      "  6.5421977e-05 1.0735389e-04 3.1455253e-05 9.9622750e-01 1.1101955e-03]\n",
      " [8.7358582e-01 2.2668472e-04 7.0943743e-02 1.8979706e-02 4.8145838e-03\n",
      "  1.2548362e-02 1.3082965e-02 9.8652847e-04 3.3209126e-03 1.5105966e-03]]\n"
     ]
    }
   ],
   "source": [
    "test_predict = model.predict_generator(test_generator,\n",
    "                                       workers=10, #并行度\n",
    "                                       use_multiprocessing=True, #True表示使用多进程， False表示使用10个线程\n",
    "                                      )\n",
    "\n",
    "# print(test_predict)\n",
    "print(test_predict.shape)# test_predict是一个300000 * 10 的矩阵\n",
    "\n",
    "print(test_predict[0:5])#打印这个矩阵前5行概率，每一行都是一个样本的输出，即所对应的十类数据的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 1 8 0]\n"
     ]
    }
   ],
   "source": [
    "#最大概率的索引来获得它对应的类别预测\n",
    "#axis参数：对于二维向量而言，0代表对行进行最大值选取，此时对每一列进行操作；1代表对列进行最大值选取，此时对每一行进行操作\n",
    "test_predict_class_index = np.argmax(test_predict, axis=1)\n",
    "print(test_predict_class_index[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'airplane', 'automobile', 'ship', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "test_predict_class = [class_names[t] for t in test_predict_class_index]\n",
    "print(test_predict_class[0:5])\n",
    "\n",
    "#print('This is a ', test_predict_class[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成 csv文件上传到 Kaggle中\n",
    "def generate_submissions(filename, predict_class):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('id,label\\n')\n",
    "        for i in range(len(predict_class)):\n",
    "            f.write('%d,%s\\n' % (i+1, predict_class[i]))\n",
    "\n",
    "output_file = '/home/galaxy/DeepLearning/DATASETS/cifar-10/submission.csv'\n",
    "generate_submissions(output_file,test_predict_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
