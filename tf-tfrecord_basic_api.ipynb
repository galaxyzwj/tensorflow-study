{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl #画图用的库\n",
    "import matplotlib.pyplot as plt\n",
    "#下面这一句是为了可以在notebook中画图\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn   #机器学习算法库\n",
    "import pandas as pd #处理数据的库   \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow import keras   #使用tensorflow中的keras\n",
    "#import keras #单纯的使用keras\n",
    " \n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, sklearn, pd, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfrecord 为tensorflow自己创建的一种文件格式\n",
    "#TFRecord内部使用了“Protocol Buffer”二进制数据编码方案，它只占用一个内存块，只需要一次性加载一个二进制文件的方式即可，简单，快速，\n",
    "#尤其对大型训练数据很友好。而且当我们的训练数据量比较大的时候，可以将数据分成多个TFRecord文件，来提高处理效率\n",
    "\n",
    "#tfrecord里面存储的都是 tf.train.Example, Example可以是一个样本也可以是一组样本\n",
    "#每个Example里面都是一个个的feature(tf.train.Features),Features里面可以看做是dicts {”key\":tf.train.Feature}\n",
    "#对于每一个不同的Feature都有不同的格式，包括tf.train.ByteList/FloatList/Int64List\n",
    "\n",
    "#得到一个UTF-8的字符串列表\n",
    "favorite_books = [name.encode(\"UTF-8\") for name in [\"machine learning\", \"cc150\"]]\n",
    "\n",
    "favorite_books_bytelist = tf.train.BytesList(value = favorite_books)\n",
    "print(favorite_books_bytelist)\n",
    "\n",
    "hours_floatList = tf.train.FloatList(value = [15.5, 9.5, 7.0, 8.0])\n",
    "print(hours_floatList)\n",
    "\n",
    "age_int64list = tf.train.Int64List(value=[27])\n",
    "print(age_int64list)\n",
    "\n",
    "#定义的features有三个特征，分别为 \"favorite_books\" \"hours\" \"age\"\n",
    "features = tf.train.Features(\n",
    "    feature = {\n",
    "        \"favorite_books\" : tf.train.Feature(bytes_list = favorite_books_bytelist),\n",
    "        \"hours\"          : tf.train.Feature(float_list = hours_floatList),\n",
    "        \"age\"            : tf.train.Feature(int64_list = age_int64list)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.Example(features=features)\n",
    "print(example)\n",
    "\n",
    "#把example进行序列化压缩，以减小tfrecord文件的大小\n",
    "serialized_example = example.SerializeToString()\n",
    "print(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把example存到tfrecord文件中，生成一个具体的tfrecord文件\n",
    "\n",
    "import shutil\n",
    "output_dir = \"tfrecord_basic\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "filename = \"test.tfrecords\"\n",
    "filename_fullpath = os.path.join(output_dir, filename)\n",
    "with tf.io.TFRecordWriter(filename_fullpath) as writer:\n",
    "    for i in range(3):#往这个tfrecord文件中写三遍上面的序列化字符串\n",
    "        writer.write(serialized_example)\n",
    "#然后我们进入tfrecord_basic文件夹下可以看到 test.tfrecords 文件\n",
    "\n",
    "#tf.data读取tfrecord文件\n",
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    print(serialized_example_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将序列化的字符串解析还原成example\n",
    "\n",
    "#定义三个特征的具体类型\n",
    "expected_features = {\n",
    "    \"favorite_books\": tf.io.VarLenFeature(dtype=tf.string),\n",
    "    \"hours\": tf.io.VarLenFeature(dtype=tf.float32),\n",
    "    \"age\": tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "#tf.data读取tfrecord文件\n",
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialized_example_tensor in dataset:\n",
    "    example = tf.io.parse_single_example(serialized_example_tensor, expected_features)\n",
    "    print(example) #\"favorite_books\" 和 \"hours\" 都是 sparse_tensor\n",
    "\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"], default_value=b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))\n",
    "\n",
    "#     hours = tf.sparse.to_dense(example[\"hours\"], default_value=tf.float32)\n",
    "#     for hour in hours:\n",
    "#         print(hour.numpy())\n",
    "\n",
    "    print(example[\"age\"].numpy())\n",
    "        \n",
    "#{'favorite_books': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f56d0ffba90>, 'hours': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f57512479b0>, 'age': <tf.Tensor: id=46, shape=(), dtype=int64, numpy=27>}\n",
    "# ”favorite_books“ 和 ”hours“ 都是 sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将tfrecord存储为压缩格式的文件\n",
    "\n",
    "filename_fullpath_zip = filename_fullpath + \".zip\"\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")#设置tfrecord文件的压缩格式\n",
    "\n",
    "with tf.io.TFRecordWriter(filename_fullpath_zip, options) as writer:\n",
    "    for i in range(3):#往这个tfrecord文件中写三遍上面的序列化字符串\n",
    "        writer.write(serialized_example)\n",
    "#然后我们进入tfrecord_basic文件夹下可以看到 test.tfrecords.zip 文件，该文件大小仅压缩前文件大小的一半\n",
    "\n",
    "\n",
    "#读取压缩的tfrecord文件\n",
    "dataset_zip = tf.data.TFRecordDataset([filename_fullpath_zip],compression_type=\"GZIP\")\n",
    "for serialized_example_tensor in dataset_zip:\n",
    "    example = tf.io.parse_single_example(serialized_example_tensor, expected_features)\n",
    "    print(example) #\"favorite_books\" 和 \"hours\" 都是 sparse_tensor\n",
    "\n",
    "    books = tf.sparse.to_dense(example[\"favorite_books\"], default_value=b\"\")\n",
    "    for book in books:\n",
    "        print(book.numpy().decode(\"UTF-8\"))\n",
    "\n",
    "#     hours = tf.sparse.to_dense(example[\"hours\"], default_value=tf.float32)\n",
    "#     for hour in hours:\n",
    "#         print(hour.numpy())\n",
    "\n",
    "    print(example[\"age\"].numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "######## 将前面的csv文件读取出来存入到tfrecords文件中\n",
    "#\n",
    "source_dir = \"./generate_csv\"\n",
    "# print(os.listdir(source_dir))\n",
    "\n",
    "def get_filename_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)#获取所有文件的名称\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):#检测字符串是否以指定的前缀开始\n",
    "            results.append(os.path.join(source_dir,filename))\n",
    "    return results\n",
    "\n",
    "\n",
    "train_filenames = get_filename_by_prefix(source_dir,\"train\")\n",
    "valid_filenames = get_filename_by_prefix(source_dir,\"valid\")\n",
    "test_filenames  = get_filename_by_prefix(source_dir,\"test\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line, n_fields=9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parse_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parse_fields[:-1])#前8个数据为x，需要区分出来\n",
    "    y = tf.stack(parse_fields[-1:])#倒数第一个数据\n",
    "    return x, y\n",
    "\n",
    "###读取 csv 文件并形成一个datasets\n",
    "#1.filename -> dataset\n",
    "#2.read file -> dataset ->datasets ->merge\n",
    "#3.parse csv\n",
    "def csv_reader_dataset(filenames, n_readers=5, \n",
    "                       batch_size=32, n_parse_threads=5, \n",
    "                       shuffle_buffer_size=10000):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()#repeat()表示 数据集遍历的次数，无参数表示遍历无数次\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)#将数据打乱，数值越大，混乱程度越大\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)#一次性取出 batch_size 个数据\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames, batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames, batch_size = batch_size)\n",
    "test_set  = csv_reader_dataset(test_filenames, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将获取到的样本数据转换为tf.train.Example格式并序列化\n",
    "def serialize_example(x, y):\n",
    "    input_features = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\":tf.train.Feature(float_list=input_features),\n",
    "            \"label\":tf.train.Feature(float_list=label)\n",
    "        }\n",
    "    \n",
    "    )\n",
    "    #根据构建的features构建Example\n",
    "    example = tf.train.Example(features=features)\n",
    "    #序列化\n",
    "    return example.SerializeToString()\n",
    "\n",
    "# 该function的作用是将 前面csv_reader_dataset中返回的数据集进行转换为tfrecords\n",
    "# base_filename 表示生成的tfrecord文件基础名字\n",
    "# dataset表示 csv_reader_dataset文件生成的dataset\n",
    "# n_shards表示 生成的tfrecord文件数量\n",
    "# steps_per_share表示 每一个csv文件在dataset上运行次数\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset, n_shards, steps_per_shard, compression_type=None):\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    all_filenames = []\n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath='{}_{:05d}-of-{:05d}'.format(base_filename, shard_id, n_shards)#设置文件全名称\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.write(serialize_example(x_example, y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.最终实现将数据写入到tfrecords\n",
    "n_shards = 20\n",
    "\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880  // batch_size // n_shards\n",
    "test_steps_per_shard  = 5170  // batch_size // n_shards\n",
    "\n",
    "\n",
    "import shutil\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename  = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filename = csv_dataset_to_tfrecords(train_basename, train_set, n_shards, train_steps_per_shard,None)\n",
    "valid_tfrecord_filename = csv_dataset_to_tfrecords(valid_basename, valid_set, n_shards, train_steps_per_shard,None)\n",
    "test_tfrecord_filename  = csv_dataset_to_tfrecords(test_basename,  test_set,  n_shards, train_steps_per_shard,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.最终实现将数据写入到被压缩过的tfreocrds文件中\n",
    "n_shards = 20\n",
    "\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880  // batch_size // n_shards\n",
    "test_steps_per_shard  = 5170  // batch_size // n_shards\n",
    "\n",
    "import shutil\n",
    "output_dir = \"generate_tfrecords_zip\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename  = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filename = csv_dataset_to_tfrecords(train_basename, train_set, n_shards, train_steps_per_shard, compression_type=\"GZIP\")\n",
    "valid_tfrecord_filename = csv_dataset_to_tfrecords(valid_basename, valid_set, n_shards, train_steps_per_shard, compression_type=\"GZIP\")\n",
    "test_tfrecord_filename  = csv_dataset_to_tfrecords(test_basename,  test_set,  n_shards, train_steps_per_shard, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(train_tfrecord_filename)\n",
    "pprint.pprint(valid_tfrecord_filename)\n",
    "pprint.pprint(test_tfrecord_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来读取前面生成的tfrecords生成数据集然后传入模型中进行训练\n",
    "\n",
    "expected_features = {\n",
    "    \"input_features\":tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\":tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialize_example):\n",
    "    example = tf.io.parse_single_example(serialize_example, expected_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "def tfrecords_reader_dataset(filenames, n_readers=5,batch_size=32, n_parse_threads=5, shuffle_buffer_size=10000): \n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()#repeat()表示 数据集遍历的次数，无参数表示遍历无数次\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TFRecordDataset(filename, compression_type=\"GZIP\"),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)#将数据打乱，数值越大，混乱程度越大\n",
    "    dataset = dataset.map(parse_example, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)#一次性取出 batch_size 个数据\n",
    "    return dataset\n",
    "\n",
    "tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filename, batch_size=3)\n",
    "for x_batch, y_batch in tfrecords_train.take(2):\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "tfrecord_train_set = tfrecords_reader_dataset(train_tfrecord_filename, batch_size=batch_size)\n",
    "tfrecord_valid_set = tfrecords_reader_dataset(valid_tfrecord_filename, batch_size=batch_size)\n",
    "tfrecord_test_set = tfrecords_reader_dataset(test_tfrecord_filename, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\",input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=5,min_delta=1e-3)\n",
    "]\n",
    "\n",
    "history=model.fit(tfrecord_train_set,\n",
    "                  validation_data = tfrecord_valid_set,\n",
    "                  steps_per_epoch = 11160 // batch_size,\n",
    "                  validation_steps = 3870 // batch_size,\n",
    "                  epochs = 100,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tfrecord_test_set, steps= 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
